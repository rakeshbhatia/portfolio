<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Project - Data Scraping for Stock Market Analysis</title>

    <!-- Bootstrap Core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="css/clean-blog.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!--<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>

    <link rel="stylesheet" href="highlight/styles/default.css">
    <script src="highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>-->

    <link href="css/prism.css" rel="stylesheet" />
    <link href="css/syntax.css" rel="stylesheet" />

    <!--<link rel="stylesheet" href="css/pygments14.css" type="text/css" />-->
    <!--<link rel="stylesheet" href="css/pygments.css" type="text/css" />-->
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    Menu <i class="fa fa-bars"></i>
                </button>
                <!-- <a class="navbar-brand" href="index.html">Start Bootstrap</a> -->
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="index.html">Home</a>
                    </li>
                    <li>
                        <a href="about.html">About</a>
                    </li>
                    <!--<li>
                        <a href="project1.html">Project 1</a>
                    </li>-->
                    <!--<li>
                        <a href="contact.html">Contact</a>
                    </li>-->
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('img/home-bg.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="page-heading">
                        <h1>Python Blog</h1>
                        <hr class="small">
                        <span class="subheading">Stock Market Data Scraping with BeautifulSoup</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p><a href="https://www.crummy.com/software/BeautifulSoup/" style="color: teal">BeautifulSoup</a>
                  is an elegant Python library which enables you to "scrape" or extract content from the web. It
                  does this by allowing you to create a BeautifulSoup object from a website's URL, which subsequently
                  contains the html content for that particular website. Once a BeautifulSoup object is created,
                  you can use its member functions to search for the content that you are interested in, along with
                  a variety of other things. Of course, this requires inspecting the html source code beforehand so
                  that you know which tags to look for.</p>
                <p>As an initial exercise to test out BeautifulSoup's features, I wanted to apply it to the stock market.
                  I was interested in generating a list of all dividend stocks in Nasdaq, S&P 500, and the dow. I figured
                  this was a good application of web scraping with Python - helping me extract mass amounts of data from
                  the web which would be too tedious to do manually. In this case, I wanted a list of all the top dividend
                  stocks, along with their market cap and 52-week price % change. </p>
                <p>I decided to use <a href="http://www.barrons.com/" style="color: teal">Barron's</a> stock website, since
                  it contained all the information above. Additionally, it was fully rendered in HTML, helping me avoid an
                  extra layer of detail by not having to scrape JavaScript content. However, note that many websites don't
                  like being scraped by automated scripts or programs, so we must take care to introduce delays in between
                  our URL requests and not overload the website with too many requests in a short period of time. After all,
                  we might be collecting information on hundreds or even thousands of different stocks at once.</p>
                <p>Before visiting Barron's, we first need a list of all the components in a particular stock index that we
                  are targeting. This list can be easily obtained from a source like Wikipedia, which has a page entirely
                  devoted to listing all the companies in the S&P 500 and Dow 30. It does not have one for Nasdaq, however,
                  so we need to get a little creative to obtain the Nasdaq stock list. Luckily Nasdaq is nice enough to provide
                  such a list, downloadable in CSV format, right off their site. Therefore, web scraping will only be required
                  to get the components for the other two indexes. Both Wiki pages for S&P and the Dow are similarly structured,
                  so let's go ahead and visit the S&P 500 components wikipedia <a href="https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
                  style="color: teal">page</a>. Right-click &#8594; inspect to pull up the following view:</p>
                <img src="img/wiki-screenshot.jpg" alt="wiki inspect screenshot" style='height: 100%; width: 100%; object-fit: contain'/>
                <p>Inspecting the HTML source code reveals that there is a table element
                  that contains a bunch of table-row (tr) elements. Each tr element is devoted to one company, and it is made up of multiple
                  table-data (td) elements. These td elements contain the company name and ticker symbol which we will need to store in some
                  type of data structure before looking up the stock quote in Barron's. A dictionary works well for this.
                  Now let's take a look at Barron's website. Enter the stock quote for a company of your choice and right-click &#8594; inspect
                  again:</p>
                <img src="img/barrons-screenshot.jpg" alt="barron's inspect screenshot" style='height: 100%; width: 100%; object-fit: contain'/>
                <p>Notice that the data for a stock quote is enclosed within a div belonging to the class "nutrition"...ðŸ¤” (don't ask me,
                  I'm not a Barron's web developer). Within this appropriately named div, we see another table element containing td and
                  tr elements which have our specific pieces of data. Notice a pattern here - numbers are frequently displayed using table
                  elements in HTML, so you will get used to recognizing this as you get more experience with BeautifulSoup. Now that we've
                  examined the HTML, we can proceed to start writing our web scraper. Before we can do anything, we will first need to declare
                  the required imports. Let's add these to the top of our file.</p>
                <pre><code class="language-python">"""Program to collect and present stock market data for Nasdaq, S&P 500, and Dow 30 indexes."""
import os
import sys
import math
import csv
import time
import bisect
import urllib2
import string
import requests
import random
import bs4
from bs4 import BeautifulSoup</code></pre>
                <p>As I mentioned before, we'll need to obtain information separately for each of the major indexes that that we want to collect
                  stock data for. Therefore, we can create a common base class called <code>Index</code> which stores information
                  on stocks that belong to it. Here is the <code>Index</code> class definition:</p>
                <pre><code class="language-python">class Index:
    def __init__(self, name, index_link):
        self.name = name
        self.index_link = index_link
        self.index_dict = {}
        self.stock_list = []
        self.stock_data = []
        self.out_file = '../docs/' + name.lower() + '-dividend-stocks-sorted.csv'

    def create_dict(self):
        if self.name == 'Nasdaq':
            self.create_dict_from_csv()
        elif self.name == 'S&P 500' or self.name == 'Dow 30':
            self.create_dict_from_web()</code></pre>
                <p>The <code>create_dict()</code> member function handles the separate mechanisms for generating stocks associated with the three
                  indexes. Essentially, a new dictionary called <code>index_dict</code> is initialized in the constructor, with the company's ticker
                  symbol being the key value that matches to a particular company name. This will enable easy lookup for the subsequent web scraping
                  operations. Notice that <code>create_dict()</code> is choosing between two helper functions depending on the index name. Let's take
                  a look at the first one, <code>create_dict_from_csv()</code>, which handles the Nasdaq part.
                <pre><code class="language-python">def create_dict_from_csv(self):
    with open(self.index_file) as csv_file:
        read_csv = csv.reader(csv_file, delimiter=',')
        for row in read_csv:
            if row[1].find('iShares') == -1 and row[1].find('iPath') == -1:
                self.index_dict[row[0]] = row[1]</code></pre>
                <p>This method handles the opening of <code>self.index_file</code>, the CSV file containing Nasdaq's dividend
                  stocks. However, one thing to keep in mind is that there are also ETFs and other funds associated with a particular
                  index; we are not interested in obtaining data for these, only for actual companies. This is why I have the <code>if
                  </code> statement - it helps filter out some, but not all, of these unwanted funds from the CSV file (those identifiable
                  with iShares or iPath). Additional filtering for harder-to-identify ETFs and funds will be done in a later step. Now
                  let's take a look at the next method, <code>create_dict_from_web()</code>, to see how to create a BeautifulSoup
                  object and parse it.</p>
                <pre><code class="language-python">def create_dict_from_web(self):
    # Create a new URL request
    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'
    headers = { 'User-Agent' : user_agent }
    url = self.index_link
    req = urllib2.Request(url, headers=headers)

    # Catch potential URL error
    try:
        response = urllib2.urlopen(req)
    except urllib2.URLError as e:
        print e.reason

    # Create BeautifulSoup object
    self.soup = BeautifulSoup(response, 'html.parser')

    #store the list of components in a dictionary by ticker symbol
    if self.name == 'S&P 500':
        for a in self.soup.findAll('table', {'class': 'wikitable sortable'}, limit=1):
            for b in a.findAll('tr'):
                count = 1
                for c in b.findAll('td', limit=2):
                    if count == 1:
                        stock_symbol = c.text
                        count += 1
                    elif count == 2:
                        company_name = c.text
                        self.index_dict[stock_symbol] = company_name
                        count = 1
    elif self.name == 'Dow 30':
        for a in self.soup.findAll('table', {'class': 'wikitable sortable'}):
            for b in a.findAll('tr'):
                count = 1
                for c in b.findAll('td', limit=3):
                    if count == 1:
                        company_name = c.text
                        count += 1
                    elif count == 2:
                        count += 1
                    elif count == 3:
                        stock_symbol = c.text
                        self.index_dict[stock_symbol] = company_name
                        count = 1</code></pre>
                <p>Don't forget that many websites don't like being browsed or scraped by programs. When <code>urllib2</code> tries to
                get a website, the website sees <code>urllib2</code> as <code>Python-urllib/x.y</code> (<code>x</code> and <code>y</code>
                refer to the major and minor version numbers, respectively, for the Python release). However, this identifier can confuse
                the site since it does not identify with any browser. It is possible to set a <code>user_agent</code> header so that <code>
                urllib2</code> can identify itself as one of the standard browsers. In our case the request will identify itself as a version
                of Internet Explorer with the dictionary of headers passed to <code>urllib2.Request()</code>. This can help avoid unexpected
                HTTP errors, since the website we are scraping will think <code>urllib2</code> is just another standard web browser.</p>
                <p>Enclosing the call to <code>urllib2.urlopen()</code> within a <code>try-except</code> block will help us catch any exceptions
                that may occur when trying to access the Wikipedia page. If <code>urllib2.urlopen()</code> is successful, we can proceed to create
                our BeautifulSoup object, <code>self.soup</code>. We use <code>urllib2.urlopen()</code> to fetch a URL-like object from the website
                that we want to scrape, and then store it in the variable <code>response</code>. This URL-like object is then passed in to the <code>
                BeautifulSoup()</code> constructor to create the <code>self.soup</code> object, which contains our desired html document in the form
                of a nested data structure. Once we have this object, we can use its <code>findAll()</code> member function to locate the html tags
                containing the content we want to scrape. In this case, we need the company name and stock ticker symbol.</p>
                <p>Inspecting the html source code for the Wikipedia page listing the S&P 500 components reveals that there is a table element
                that contains a bunch of table row (tr) elements. Each tr element is devoted to one company, and it is made up of multiple
                table-data (td) elements. These td elements contain the company name and ticker symbol which we use to populate our
                dictionary. We must use a <code>count</code> variable to keep track of where we are while looping through the tr elements;
                the two pieces of information that we want are stored in the 1st and 2nd td elements belonging to a particular tr element
                (1st and 3rd for the Dow Components Wikipedia page). Once we have finished adding all the stocks to our dictionary, we can
                proceed to looking up the stock quotes in an automated manner. The next step is to add stocks to the Index's stock list.
                Let's define our Stock class before we populate <code>self.stock_list</code>.</p>
                <pre><code class="language-python">class Stock:
    def __init__(self, symbol, company):
        self.symbol = symbol
        self.company = company
        self.data = []</code></pre>
                <p>The constructor takes two required arguments, <code>symbol</code> and <code>company</code> (which, if you recall, we are deriving
                  from the <code>Index</code> member function <code>create_dict())</code>. Additionally, <code>self.data</code> is being initialized
                  to an empty list. This will store the relevant data that we are interested in extracting for a particular stock. Now that we have a
                  skeleton for our <code>Stock</code>object, let's switch gears back to the <code>Index</code> class definition and define a method to
                  add stocks to our list.</p>
                <pre><code class="language-python">def add_stocks(self):
    for key, value in self.index_dict.items():
        new_stock = Stock(key, value)
        new_stock.query_stock_symbol()
        if new_stock.div_yield != None:
            self.stock_list.append(new_stock)
    #Sort the stock list by yield amount, in desecending order
    self.stock_list.sort(key=lambda stock: stock.div_yield, reverse=True)</code></pre>
                <p>We can easily initialize each new <code>Stock</code> object by using the standard technique for iterating through our dictionary,
                  looping through <code>self.index_dict.items()</code>. This enables us to use each key-value pair as arguments to the <code>Stock
                  </code> constructor. However, we need to define additional functionality for the <code>Stock</code> class, since we need to
                  scrape the stock data from Barron's. Let's define a <code>query_stock_symbol()</code> method in the <code>Stock</code> class:</p>
                <pre><code class="language-python">def query_stock_symbol(self):
    # Add wait times in between getting each stock's data to prevent overload
    wait_time = round(max(5, 10 + random.gauss(0,3)), 2)
    time.sleep(wait_time)

    # Check for two different Barron's URLs
    url = 'http://www.barrons.com/quote/stock/us/xnas/%s' % (self.symbol)
    page = requests.get(url)
    if page.status_code == 404:
        url = 'http://www.barrons.com/quote/stock/us/xnys/%s?mod=DNH_S' % (self.symbol)

    # Create a new URL request
    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'
    headers = { 'User-Agent' : user_agent }
    req = urllib2.Request(url, headers=headers)

    # Catch potential URL error
    try:
        response = urllib2.urlopen(req)
    except urllib2.URLError as e:
        print e.reason

    # Create BeautifulSoup object
    self.soup = BeautifulSoup(response, 'html.parser')

    # Find stock price
    for a in self.soup.findAll('span', {'class':'market__price'}):
        stock_price_str = a.text.replace(',', '')
        if stock_price_str != 'N/A':
            self.stock_price = float(stock_price_str)
        else:
            self.stock_price = None

    # Append remaining data
    for a in self.soup.findAll('div', {'class': 'nutrition'}):
        for b in a.findAll('td'):
            self.data.append(b.text)

    # Extract remaining data
    self.market_cap = None
    for i in xrange(0, len(self.data)):
        if self.data[i] == 'Market Value':
            self.market_cap = self.data[i+1]
        elif self.data[i] == 'Ytd net Change':
            self.ytd_net_change_str = self.data[i+1].strip('%')
            if self.ytd_net_change_str != 'N/A':
                self.ytd_net_change = float(self.ytd_net_change_str)
            else:
                self.ytd_net_change = -1
        elif self.data[i] == 'Div & Yield':
            div_amount_str = self.data[i+1].split(' (')[0].strip(' ')
            div_amount_str = div_amount_str.strip('$')
            div_yield_str = self.data[i+1].split(' (')[1].strip(')')
            div_yield_str = div_yield_str.strip('%')
            if div_amount_str != 'N/A':
                self.div_amount = float(div_amount_str)
                self.div_yield = float(div_yield_str)
            else:
                self.div_amount = None
                self.div_yield = None</code></pre>
                <p>Notice that the first part is similar to the <code>create_dict_from_web()</code> method in our <code>Index</code> class,
                where we create a <code>self.soup</code> object for parsing. The only difference is the code below:</p>
                <pre><code class="language-python"># Check for two different Barron's URLs
url = 'http://www.barrons.com/quote/stock/us/xnas/%s' % (self.symbol)
page = requests.get(url)
if page.status_code == 404:
    url = 'http://www.barrons.com/quote/stock/us/xnys/%s?mod=DNH_S' % (self.symbol)</code></pre>
                <p>We have to choose the <code>url</code> for each individual stock query. Barron's has two different url versions that
                it uses for its stock quotes, so we check <code>requests.get()</code> on one version to see if it was successful. If
                <code>page.status_code</code> returns 404, then we reset <code>url</code> to the other version. Python's string <code>
                replace</code> functionality enables us to easily modify the <code>url</code> string for each ticker symbol, since the
                symbol is contained within the URL for each stock quote. Once our <code>url</code> string is defined, we create a
                BeautifulSoup object the same way as before. Let's take a look at what happens after that.</p>
                <pre><code class="language-python"># Find stock price
for a in self.soup.findAll('span', {'class':'market__price'}):
    stock_price_str = a.text.replace(',', '')
    if stock_price_str != 'N/A':
        self.stock_price = float(stock_price_str)
    else:
        self.stock_price = None

# Append remaining data
for a in self.soup.findAll('div', {'class': 'nutrition'}):
    for b in a.findAll('td'):
        self.data.append(b.text)</code></pre>
                <p>Again, we are using <code>findAll()</code> to locate the information we need. In this case, the price of the stock is located
                in a <code>span</code> element identified by the class market__price. Since numbers $1000 or greater are listed with a comma on
                the site, we need to remove the comma and replace it with whitespace since Python doesn't understand numbers with a comma. Then,
                we can convert the <code>string</code> to a <code>float</code>. The second loop just gathers the remaining data, and stores it in
                the list <code>data</code> that we declared earlier. We process this list in the next part below.</p>
                <pre><code class="language-python"># Extract remaining data
self.market_cap = None
for i in xrange(0, len(self.data)):
    if self.data[i] == 'Market Value':
        self.market_cap = self.data[i+1]
    elif self.data[i] == 'Ytd net Change':
        self.ytd_net_change_str = self.data[i+1].strip('%')
        if self.ytd_net_change_str != 'N/A':
            self.ytd_net_change = float(self.ytd_net_change_str)
        else:
            self.ytd_net_change = -1
    elif self.data[i] == 'Div & Yield':
        div_amount_str = self.data[i+1].split(' (')[0].strip(' ')
        div_amount_str = div_amount_str.strip('$')
        div_yield_str = self.data[i+1].split(' (')[1].strip(')')
        div_yield_str = div_yield_str.strip('%')
        if div_amount_str != 'N/A':
            self.div_amount = float(div_amount_str)
            self.div_yield = float(div_yield_str)
        else:
            self.div_amount = None
            self.div_yield = None</code></pre>
                <p>Here, we extract the remaining data - the market cap, ytd net change, and dividend/yield. Let's jump back to the
                Index class where we defined our <code>add_stocks()</code> function.</p>
                <pre><code class="language-python">def add_stocks(self):
    for key, value in self.index_dict.items():
        new_stock = Stock(key, value)
        new_stock.query_stock_symbol()
        if new_stock.div_yield != None:
            self.stock_list.append(new_stock)
    #Sort the stock list by yield amount, in desecending order
    self.stock_list.sort(key=lambda stock: stock.div_yield, reverse=True)</code></pre>
                <p>Now that we have queried our stock symbol, we can add the stock to <code>stock_list</code>, as long as it is a
                dividend stock. We are not interested in non-dividend stocks for this exercise. Once we populate the list, we need
                to sort it, which can be easily achieved using Python's <code>sort</code> feature. This method takes a key value as
                its primary argument. Notice that we use a <code>lambda</code> function which evaluates to the value of <code>div_yield
                </code> as the <code>key</code> value. Additionally, we set <code>reverse=True</code> so that the list is
                sorted in descending order (the option can be left empty if you want ascending order).</p>
                <p>Great! We've covered everything that involes scraping the requisite data from Barron's website. However, now we want
                to see the data printed out in some form - like a CSV file. The following method, <code>from_dict_to_csv()</code>,
                handles this functionality.</p>
                <pre><code class="language-python">def from_dict_to_csv(self):
    self.add_stocks()
    self.headings = ['Company','Symbol','Current Price','Market Cap','Dividend', 'Yield', '52-Week Return']
    for i in xrange(0, len(self.stock_list)):
        new_dict = {}
        new_dict['Company'] = self.stock_list[i].company
        new_dict['Symbol'] = self.stock_list[i].symbol
        new_dict['Current Price'] = self.stock_list[i].stock_price
        new_dict['Market Cap'] = self.stock_list[i].market_cap
        new_dict['Dividend'] = self.stock_list[i].div_amount
        if self.stock_list[i].div_yield != None:
            new_dict['Yield'] = str(self.stock_list[i].div_yield) + '%'
        else:
            new_dict['Yield'] = 'N/A'
        if self.stock_list[i].ytd_net_change != None:
            new_dict['52-Week Return'] = str(self.stock_list[i].ytd_net_change) + '%'
        else:
            new_dict['52-Week Return'] = 'None'
        self.stock_data.append(new_dict)

    try:
        with open(self.out_file, "wb") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=self.headings, dialect='excel', delimiter=',', quoting=csv.QUOTE_NONNUMERIC)
            writer.writeheader()
            for data in self.stock_data:
                writer.writerow(data)

    except IOError as (errno, strerror):
        print "I/O error({0}): {1}".format(errno, strerror)</code></pre>
                <p>Here, we create a dictionary of headers with the column titles that we want in our CSV file. Then, we add each dictionary
                (which corresponds to one stock) to a list of dictionaries, <code>stock_data</code>. Once our list of dictionaries is fully
                populated, we can proceed to use Python's <code>csv.DictWriter()</code> feature to print the data to our output file. All the
                functionality discussed thus far has been tied together in the rest of the program, shown below:</p>
                <pre><code class="language-python">def generate_dividend_stocks():
    nasdaq_file = '../docs/dividend-stocks-nasdaq.csv'
    nasdaq_index = Index('Nasdaq', nasdaq_file)
    nasdaq_index.create_dict()
    nasdaq_index.from_dict_to_csv()

    sp_link = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
    sp_index = Index('S&P 500', sp_link)
    sp_index.create_dict()
    sp_index.from_dict_to_csv()

    dow_link = 'https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average#Components'
    dow_index = Index('Dow 30', dow_link)
    dow_index.create_dict()
    dow_index.from_dict_to_csv()

    def main():
        generate_dividend_stocks()

    if __name__ == '__main__':
        sys.exit(main())</code></pre>
                <p>Everything is encapsulated underneath the function <code>generate_dividend_stocks()</code>, in which we create the three
                separate stock indexes and print out a separate CSV file for each one. Running the program, we can generate an output file
                like the one shown below for the Dow 30. Note that the stocks are listed in order of decreasing yield %.</p>
                <img src="img/stock_list.jpg" alt="dow 30 stocks list" style='height: 100%; width: 100%; object-fit: contain'/>
                <p>Be sure to check out the <a href="https://github.com/rakeshbhatia/stock-market-data" style="color:teal">Github</a> repository
                if you want to download the source code for yourself. Thanks for reading!</p>
            </div>
        </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; Rakesh Bhatia 2017</p>
                </div>
            </div>

            <div class="col-lg-8">
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

    <script src="js/prism.js"></script>
</body>

</html>
